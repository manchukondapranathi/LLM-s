# Mental Health Detection Using Large Language Models

## Introduction
Mental health is a growing global concern, with conditions like depression, anxiety, and stress becoming more prevalent due to modern lifestyles and societal pressures. Traditional mental health consultations often require face-to-face interactions with professionals, which can be challenging due to time constraints and accessibility issues. This project aims to develop an AI-powered model that can generate accurate responses to mental health-related queries using Large Language Models (LLMs) like ChatGPT and Gemini. By leveraging AI, the goal is to provide accessible and private mental health support.

## Objectives
The study focuses on evaluating the effectiveness of LLMs in recognizing and understanding mental health concerns in text-based interactions. Key objectives include:

- Linguistic Feature Analysis: Investigate linguistic characteristics such as syntactic, semantic, and discourse patterns in mental health-related conversations.
- Performance Evaluation: Assess the accuracy and reliability of ChatGPT and Gemini in detecting language patterns indicative of mental health concerns.
- Embedding Comparisons: Compare embeddings generated by ChatGPT and Gemini against a ground truth dataset to evaluate their coherence.
- Benchmarking with Cosine Similarity: Use cosine similarity scores to measure the models' ability to detect mental health-related language.
- Ethical Considerations: Address concerns related to privacy, data security, bias, and informed consent when using LLMs for mental health detection.

## Research Design
The research is structured into the following phases:

1. Data Collection: The "mental-health-conversational-data" dataset from Hugging Face is utilized. This dataset includes interactions between therapists/doctors and patients discussing mental health concerns.
2. Data Preprocessing: The dataset is cleaned by removing null values, empty strings, and "None" values to ensure data quality.
3. Exploratory Data Analysis (EDA): Word clouds and value counts are generated to analyze conversation characteristics and identify key linguistic patterns.
4. Response Generation: ChatGPT and Gemini are used to generate AI-driven responses based on the dataset context.
5. Embedding Conversion: The generated responses are converted into numerical embeddings using BERT.
6. Cosine Similarity Score Calculation: Cosine similarity is used to compare the embeddings and quantify the similarity between AI-generated responses and ground truth responses.

## Data Collection and Analysis
The dataset consists of 661 rows and three columns: context, knowledge, and response. This dataset provides insight into mental health support through interactions between therapists and patients.

### Exploratory Data Analysis (EDA)
EDA helps in understanding the dataset structure and conversation trends. Key methods used:
- Word Clouds: Visualize the most frequently used words in conversations.
- Value Counts: Analyze the unique categories in the "Knowledge" column to understand response diversity.

## Hypotheses
The study hypothesizes that **ChatGPT will outperform Gemini** in generating mental health-related responses. This is tested by comparing cosine similarity scores between original responses and AI-generated responses.

## Model Analysis and Evaluation
- Response Generation: AI models (ChatGPT and Gemini) generate responses based on dataset context.
- Embedding Conversion: Responses are transformed into embeddings using BERT.
- Cosine Similarity Comparison: AI-generated embeddings are compared with ground truth embeddings using cosine similarity.

## Results
- Cosine Similarity Scores:
  - ChatGPT: **93%**
  - Gemini: **88%**
- Data Visualization:
  - Scatter plots, bar charts, and heatmaps illustrate differences in model performance and response similarity.

## Conclusion
The study concludes that ChatGPT demonstrates higher accuracy and consistency in identifying linguistic patterns associated with mental health issues compared to Gemini. ChatGPT's performance makes it a promising tool for AI-assisted mental health support. However, further research is necessary to address ethical considerations, data privacy, and potential real-world applications.

---

### Future Scope
- Enhancing model accuracy with a larger dataset.
- Exploring bias mitigation techniques in AI-driven mental health analysis.
- Investigating real-time applications of AI in mental health support systems.

This project highlights the potential of AI-powered LLMs in mental health detection while emphasizing the importance of ethical considerations and continuous improvements.

